{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "edc36ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import csv\n",
    "import random\n",
    "from openai import OpenAI\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ef48325",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# initialize client once\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "# model name for output files\n",
    "model_name = \"gpt-5\"\n",
    "\n",
    "# folders\n",
    "BQA_FOLDER = \"LogicBenchFiles/Eval .json Files/BQA\"\n",
    "MCQA_FOLDER = \"LogicBenchFiles/Eval .json Files/MCQA\"\n",
    "EASTERN_BQA_FOLDER = \"LogicBenchFiles/Eval .json Files/Eastern BQA\"\n",
    "EASTERNDATA_BQA_FOLDER = \"LogicBenchFiles/Eval .json Files/EasternData BQA\"\n",
    "\n",
    "# for consistent end prompts\n",
    "ENDPROMPT_BQA = \"Only answer 'yes' or 'no' do not provide any additional characters like punctuation or explanation just yes or no thats it.:\"\n",
    "ENDPROMPT_MCQA = \"Do not provide any additional characters like punctuation or explanation just the choice thats it (ex. Choice_1, Choice_2, Choice_3, Choice_4).:\"\n",
    "ENDPROMPT_EasternData = (\n",
    "    \"If the question is a yes or no question, only answer 'yes' or 'no'. \"\n",
    "    \"If it is a count or recall question, answer with only the exact number or the exact answer. \"\n",
    "    \"Do not include any punctuation, explanations, or extra words â€” just the direct answer.\\n\\n\"\n",
    "    \"For recall questions that require listing course sections with times and days, format exactly like this:\\n\"\n",
    "    \"Sec 1, tr, 8:00am to 9:15am, Sec 2, mwf, 8:00am to 8:50am, Sec 3, mwf, 9:00am to 9:50am\\n\"\n",
    "    \"No extra punctuation, no 'and', no trailing periods.\\n\\n\"\n",
    "    \"For recall questions that require listing names, use the professor's full name exactly as displayed in the dataset, \"\n",
    "    \"in the same order they appear. Separate names with a comma and a space, like this:\\n\"\n",
    "    \"Jane Doe, Jack H. Ryan, Beth Stevens\\n\"\n",
    "    \"Do not reorder, reformat, or add extra text of any kind.\"\n",
    ")\n",
    "\n",
    "# Dataset file paths\n",
    "DATACONPath = \"intermediateFiles/test10Spring2025_contactinfo_2025-04-02.csv\"\n",
    "DATACORPath = \"intermediateFiles/Test8Spring2025_Instructor_Course_Schedule_2025-05-07.csv\"\n",
    "DATAOHPath = \"intermediateFiles/6TestSpring2025_Exclusive_Hours_2025-05-07.csv\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2615c094",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# function to process BQA files\n",
    "def process_bqa(file_path):\n",
    "    with open(file_path, \"r\") as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    rows = []\n",
    "    for current_q in data[\"samples\"]:\n",
    "        for qa1 in current_q[\"qa_pairs\"]:\n",
    "            prompt = current_q[\"context\"] + \"\\n\" + qa1[\"question\"] + \"\\n\" + ENDPROMPT_BQA\n",
    "\n",
    "            response = client.responses.create(\n",
    "                model= model_name,\n",
    "                input=prompt,\n",
    "                store=False,\n",
    "            )\n",
    "            chat_gpt_answer = response.output_text.lower().strip()\n",
    "\n",
    "            rows.append({\n",
    "                \"Prompt\": prompt,\n",
    "                \"Context\": current_q[\"context\"],\n",
    "                \"Question\": qa1[\"question\"],\n",
    "                \"Endprompt\": ENDPROMPT_BQA,\n",
    "                \"Correct answer\": qa1[\"answer\"],\n",
    "                \"ChatGPT answer\": chat_gpt_answer\n",
    "            })\n",
    "    return rows\n",
    "\n",
    "\n",
    "# function to process MCQA files\n",
    "def process_mcqa(file_path):\n",
    "    with open(file_path, \"r\") as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    rows = []\n",
    "    for current_q in data[\"samples\"]:\n",
    "        prompt = current_q[\"context\"] + \"\\n\" + current_q[\"question\"] + \" Select the best answer from the choices below:\\n\\n\"\n",
    "        for key, value in current_q[\"choices\"].items():\n",
    "            prompt += f\"{key}: {value}\\n\"\n",
    "        prompt += \"\\n\" + ENDPROMPT_MCQA\n",
    "\n",
    "        response = client.responses.create(\n",
    "            model= model_name,\n",
    "            input=prompt,\n",
    "            store=False,\n",
    "        )\n",
    "        chat_gpt_answer = response.output_text.lower().strip()\n",
    "\n",
    "        rows.append({\n",
    "            \"Prompt\": prompt,\n",
    "            \"Context\": current_q[\"context\"],\n",
    "            \"Question\": current_q[\"question\"],\n",
    "            \"Endprompt\": ENDPROMPT_MCQA,\n",
    "            \"Correct answer\": current_q[\"answer\"],\n",
    "            \"ChatGPT answer\": chat_gpt_answer\n",
    "        })\n",
    "    return rows\n",
    "\n",
    "\n",
    "\n",
    "def write_to_csv(file_path, rows):\n",
    "    # Extract just the filename, replace extension with _completed.csv\n",
    "    base_name = os.path.basename(file_path).replace(\".json\", f\"_{model_name}_completed.csv\")\n",
    "    # Save into the same folder as the input file\n",
    "    folder = os.path.dirname(file_path)\n",
    "    csv_file = os.path.join(folder, base_name)\n",
    "\n",
    "    with open(csv_file, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=[\"Prompt\", \"Context\", \"Question\", \"Endprompt\", \"Correct answer\", \"ChatGPT answer\"])\n",
    "        writer.writeheader()\n",
    "        writer.writerows(rows)\n",
    "    print(f\"Saved: {csv_file}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6946f034",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- new helper function to read dataset text ---\n",
    "def read_csv_as_text(file_path, max_lines=None):\n",
    "    \"\"\"Reads up to max_lines of CSV to include as reference text for the model.\"\"\"\n",
    "    try:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            lines = f.readlines()\n",
    "            # keep it small enough to avoid token overload\n",
    "            preview = \"\".join(lines[:max_lines])\n",
    "            return f\"\\n\\n[DATASET PREVIEW FROM {os.path.basename(file_path)}]\\n{preview}\\n\\n\"\n",
    "    except Exception as e:\n",
    "        return f\"\\n\\n[Could not load dataset: {e}]\\n\\n\"\n",
    "\n",
    "# --- logic for dataset choice ---\n",
    "def get_dataset_and_text(context_text):\n",
    "    context_lower = context_text.lower()\n",
    "    if \"faculty office hours\" in context_lower:\n",
    "        return DATAOHPath, read_csv_as_text(DATAOHPath)\n",
    "    elif \"course times\" in context_lower:\n",
    "        return DATACORPath, read_csv_as_text(DATACORPath)\n",
    "    elif \"contact info\" in context_lower or \"faculty contact\" in context_lower:\n",
    "        return DATACONPath, read_csv_as_text(DATACONPath)\n",
    "    else:\n",
    "        return None, \"\"\n",
    "\n",
    "def process_bqaDATA(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    rows = []\n",
    "    for current_q in data[\"samples\"]:\n",
    "        time.sleep(15)\n",
    "        dataset_path, dataset_text = get_dataset_and_text(current_q[\"context\"])\n",
    "\n",
    "        for qa1 in current_q[\"qa_pairs\"]:\n",
    "            # embed dataset text directly in the prompt\n",
    "            prompt = (\n",
    "                f\"{current_q['context']}\\n{dataset_text}{qa1['question']}\\n{ENDPROMPT_EasternData}\"\n",
    "            )\n",
    "\n",
    "            displayprompt =(\n",
    "                f\"{current_q['context']}\\n{\"' Data is inserted here' \"}{qa1['question']}\\n{ENDPROMPT_EasternData}\"\n",
    "            )\n",
    "\n",
    "            response = client.responses.create(\n",
    "                model=model_name,\n",
    "                input=prompt,\n",
    "                store=False,\n",
    "            )\n",
    "\n",
    "            chat_gpt_answer = response.output_text.lower().strip()\n",
    "\n",
    "            rows.append({\n",
    "                \"Prompt\": displayprompt,\n",
    "                \"Context\": current_q[\"context\"],\n",
    "                \"Question\": qa1[\"question\"],\n",
    "                \"Dataset Used\": dataset_path if dataset_path else \"N/A\",\n",
    "                \"Endprompt\": ENDPROMPT_EasternData,\n",
    "                \"Correct answer\": qa1[\"answer\"],\n",
    "                \"ChatGPT answer\": chat_gpt_answer\n",
    "            })\n",
    "    return rows\n",
    "\n",
    "def write_to_csv(file_path, rows):\n",
    "    base_name = os.path.basename(file_path).replace(\".json\", f\"_{model_name}_completed.csv\")\n",
    "    folder = os.path.dirname(file_path)\n",
    "    csv_file = os.path.join(folder, base_name)\n",
    "\n",
    "    with open(csv_file, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        fieldnames = [\"Prompt\", \"Context\", \"Question\", \"Dataset Used\", \"Endprompt\", \"Correct answer\", \"ChatGPT answer\"]\n",
    "        writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        writer.writerows(rows)\n",
    "    print(f\"Saved: {csv_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc4dae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# main loop for Logic Bench\n",
    "for folder, processor in [(BQA_FOLDER, process_bqa), (MCQA_FOLDER, process_mcqa)]:\n",
    "    for filename in os.listdir(folder):\n",
    "        if filename.endswith(\".json\"):\n",
    "            filepath = os.path.join(folder, filename)\n",
    "            print(f\"Processing {filepath}...\")\n",
    "            rows = processor(filepath)\n",
    "            write_to_csv(filepath, rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e327768",
   "metadata": {},
   "outputs": [],
   "source": [
    "# main loop for Eastern BQA\n",
    "for folder, processor in [(EASTERN_BQA_FOLDER, process_bqa)]:\n",
    "    for filename in os.listdir(folder):\n",
    "        if filename.endswith(\".json\"):\n",
    "            filepath = os.path.join(folder, filename)\n",
    "            print(f\"Processing {filepath}...\")\n",
    "            rows = processor(filepath)\n",
    "            write_to_csv(filepath, rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c736a65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing LogicBenchFiles/Eval .json Files/EasternData BQA\\Count.json...\n"
     ]
    },
    {
     "ename": "InternalServerError",
     "evalue": "Error code: 500 - {'error': {'message': 'An error occurred while processing your request. You can retry your request, or contact us through our help center at help.openai.com if the error persists. Please include the request ID req_78aeee4441284622831d3422c39843cc in your message.', 'type': 'server_error', 'param': None, 'code': 'server_error'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInternalServerError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m filepath \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(folder, filename)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilepath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 7\u001b[0m rows \u001b[38;5;241m=\u001b[39m \u001b[43mprocessor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m write_to_csv(filepath, rows)\n",
      "Cell \u001b[1;32mIn[3], line 44\u001b[0m, in \u001b[0;36mprocess_bqaDATA\u001b[1;34m(file_path)\u001b[0m\n\u001b[0;32m     36\u001b[0m prompt \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcurrent_q[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontext\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mdataset_text\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mqa1[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mENDPROMPT_EasternData\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     38\u001b[0m )\n\u001b[0;32m     40\u001b[0m displayprompt \u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m     41\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcurrent_q[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontext\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m Data is inserted here\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mqa1[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mENDPROMPT_EasternData\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     42\u001b[0m )\n\u001b[1;32m---> 44\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresponses\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     45\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     46\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     47\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstore\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     48\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     50\u001b[0m chat_gpt_answer \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39moutput_text\u001b[38;5;241m.\u001b[39mlower()\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[0;32m     52\u001b[0m rows\u001b[38;5;241m.\u001b[39mappend({\n\u001b[0;32m     53\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPrompt\u001b[39m\u001b[38;5;124m\"\u001b[39m: displayprompt,\n\u001b[0;32m     54\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mContext\u001b[39m\u001b[38;5;124m\"\u001b[39m: current_q[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontext\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChatGPT answer\u001b[39m\u001b[38;5;124m\"\u001b[39m: chat_gpt_answer\n\u001b[0;32m     60\u001b[0m })\n",
      "File \u001b[1;32mc:\\Users\\stevo\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\openai\\resources\\responses\\responses.py:814\u001b[0m, in \u001b[0;36mResponses.create\u001b[1;34m(self, background, conversation, include, input, instructions, max_output_tokens, max_tool_calls, metadata, model, parallel_tool_calls, previous_response_id, prompt, prompt_cache_key, reasoning, safety_identifier, service_tier, store, stream, stream_options, temperature, text, tool_choice, tools, top_logprobs, top_p, truncation, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[0;32m    777\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcreate\u001b[39m(\n\u001b[0;32m    778\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    779\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    812\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[0;32m    813\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Response \u001b[38;5;241m|\u001b[39m Stream[ResponseStreamEvent]:\n\u001b[1;32m--> 814\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    815\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/responses\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    816\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    817\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[0;32m    818\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbackground\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mbackground\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    819\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mconversation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mconversation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    820\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minclude\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43minclude\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    821\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    822\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minstructions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43minstructions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    823\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_output_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_output_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    824\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_tool_calls\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    825\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    826\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    827\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparallel_tool_calls\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    828\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprevious_response_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprevious_response_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    829\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprompt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    830\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprompt_cache_key\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_cache_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    831\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mreasoning\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    832\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msafety_identifier\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msafety_identifier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    833\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mservice_tier\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    834\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstore\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    835\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    836\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    837\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    838\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    839\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtool_choice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    840\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtools\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    841\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_logprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    842\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    843\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtruncation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    844\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    845\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    846\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresponse_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mResponseCreateParamsStreaming\u001b[49m\n\u001b[0;32m    847\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\n\u001b[0;32m    848\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mresponse_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mResponseCreateParamsNonStreaming\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    849\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    850\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    851\u001b[0m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[0;32m    852\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    853\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mResponse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    854\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    855\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mResponseStreamEvent\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    856\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\stevo\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\openai\\_base_client.py:1259\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[1;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1245\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpost\u001b[39m(\n\u001b[0;32m   1246\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1247\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1254\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1255\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m   1256\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[0;32m   1257\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[0;32m   1258\u001b[0m     )\n\u001b[1;32m-> 1259\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32mc:\\Users\\stevo\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\openai\\_base_client.py:1047\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[1;34m(self, cast_to, options, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1044\u001b[0m             err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m   1046\u001b[0m         log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1047\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1049\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m   1051\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcould not resolve response (should never happen)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;31mInternalServerError\u001b[0m: Error code: 500 - {'error': {'message': 'An error occurred while processing your request. You can retry your request, or contact us through our help center at help.openai.com if the error persists. Please include the request ID req_78aeee4441284622831d3422c39843cc in your message.', 'type': 'server_error', 'param': None, 'code': 'server_error'}}"
     ]
    }
   ],
   "source": [
    "# main loop for Eastern Dataset Questions\n",
    "for folder, processor in [(EASTERNDATA_BQA_FOLDER, process_bqaDATA)]:\n",
    "    for filename in os.listdir(folder):\n",
    "        if filename.endswith(\".json\"):\n",
    "            filepath = os.path.join(folder, filename)\n",
    "            print(f\"Processing {filepath}...\")\n",
    "            rows = processor(filepath)\n",
    "            write_to_csv(filepath, rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e749b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "import random\n",
    "import csv\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe11d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_name = \"gpt-5\"\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key= os.getenv(\"OPENAI_API_KEY\")\n",
    "    )\n",
    "\n",
    "\n",
    "response = client.responses.create(\n",
    "  model= \"gpt-4o-mini\",\n",
    "  input=\"Hello ChatGPT, What model are you?\",\n",
    "  store=True,\n",
    ")\n",
    "\n",
    "print(response.output_text);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f915f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('LogicBenchFiles/Eval .json Files/BQA/EVAL_ModusPones(BQA).json') as file :\n",
    "    questions = file.read()\n",
    "\n",
    "questions = json.loads(questions)\n",
    "questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aae3fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('LogicBenchFiles/Eval .json Files/MCQA/EVAL_ModusPones(MCQA).json') as file :\n",
    "    questions2 = file.read()\n",
    "\n",
    "questions2 = json.loads(questions2)\n",
    "questions2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "432fa506",
   "metadata": {},
   "outputs": [],
   "source": [
    "q3 = questions2['samples']\n",
    "len(q3)\n",
    "print(q3[0].keys())\n",
    "\n",
    "print(q3[0]['answer'])\n",
    "print(q3[0]['choices'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7879762e",
   "metadata": {},
   "outputs": [],
   "source": [
    "q = questions['samples']\n",
    "len(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "294fdc24",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(q[0].keys())\n",
    "\n",
    "print(q[0]['context'])\n",
    "\n",
    "q[5]['qa_pairs']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d452d11",
   "metadata": {},
   "source": [
    "Each prompt:\n",
    "- include context ('context')\n",
    "- include question ('q_pairs')\n",
    "- compare to answer ('answer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9973960",
   "metadata": {},
   "outputs": [],
   "source": [
    "q[5]['qa_pairs'][0]['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48526767",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for current_q in q:\n",
    "        \n",
    "    for qa1 in current_q['qa_pairs']:\n",
    "        prompt = '' \n",
    "        prompt += current_q['context'] + '\\n' + qa1['question'] + '\\n' + \"Only answer 'yes' or 'no' do not provide any additonal characters like punctuation or explantation just yes or no thats it.:\"\n",
    "\n",
    "        client = OpenAI(\n",
    "        api_key= os.getenv(\"OPENAI_API_KEY\")\n",
    "        )   \n",
    "\n",
    "\n",
    "        response = client.responses.create(\n",
    "            model= \"gpt-4o-mini\",\n",
    "            input= prompt,\n",
    "            store=False,\n",
    "        )\n",
    "        chat_gpt_answer = (response.output_text);\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        print(prompt)\n",
    "        num = random.random()\n",
    "        chat_gpt = ' '\n",
    "        if num > 0.6:\n",
    "            chat_gpt = 'no'\n",
    "        elif num > 0.2:\n",
    "            chat_gpt = 'yes'\n",
    "        print('ChatGPT answer:', chat_gpt_answer.lower().strip())\n",
    "        print('answer:', qa1['answer'])\n",
    "        print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b229931d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for current_q2 in q3:\n",
    "        prompt1 = '' \n",
    "        prompt1 += current_q2['context'] + '\\n' + current_q2['question'] + ' Select the best answer from the choices below:\\n\\n' \n",
    "        \n",
    "        for key, value in current_q2['choices'].items():\n",
    "            prompt1 += f\"{key}: {value}\\n\"\n",
    "\n",
    "\n",
    "        prompt1 += \"\\nDo not provide any additonal characters like punctuation or explantation just the choice thats it(ex. Choice_1, Choice_2, Choice_3, Choice_4).:\"\n",
    "\n",
    "    \n",
    "        client = OpenAI(\n",
    "        api_key= os.getenv(\"OPENAI_API_KEY\")\n",
    "        )   \n",
    "\n",
    "\n",
    "        response = client.responses.create(\n",
    "            model= \"gpt-4o-mini\",\n",
    "            input= prompt1,\n",
    "            store=False,\n",
    "        )\n",
    "        chat_gpt_answer2 = (response.output_text);\n",
    "        \n",
    "\n",
    "\n",
    "        \n",
    "        print(prompt1)\n",
    "        num = random.random()\n",
    "        chat_gpt2 = ' '\n",
    "        if num > 0.6:\n",
    "            chat_gpt2 = 'choice 1'\n",
    "        elif num > 0.2:\n",
    "            chat_gpt2 = 'choice 2'\n",
    "        print('ChatGPT answer:', chat_gpt_answer2.lower().strip())\n",
    "        print('answer:', current_q2['answer'])\n",
    "        print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
